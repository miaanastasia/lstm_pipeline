{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/miaanastasia/lstm_pipeline/blob/main/LSTM_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Wyt0FBmegn1"
      },
      "source": [
        "# **LSTM-Based Stock Price Prediction**\n",
        "\n",
        "This notebook demonstrates a stock prediction pipeline using machine learning and LSTM neural networks, with technical and market-based features, visual evaluation, and an interactive UI for exploring results.\n",
        "\n",
        "> *This program is optimized for Google Colab Pro accounts with GPU access and at least ten available computing units.*\n",
        "\n",
        "## User Guide\n",
        "\n",
        "#### Processor\n",
        "1. Click `Connect` at the top right of the notebook.\n",
        "2. Select `L4 GPU` or `T4 GPU` from the runtime options.\n",
        "3. Verify connectivity with green check mark.\n",
        "\n",
        "#### Pipeline\n",
        "1. In the Colab menu bar, go to **Runtime > Run All.**\n",
        "2. The default ticker is set to `DOW`.\n",
        "3. Wait a few moments while all cells execute.\n",
        "4. Once complete, check the task bar at the bottom for confirmation.\n",
        "\n",
        "\n",
        "> ***Tips:***\n",
        "> - Collapse sections using the bold headers for cleaner navigation.\n",
        "> - To run a specific section, hover over the `[ ]` icon below the collapsed header and click the play button.\n",
        "\n",
        "#### User Interface\n",
        "1. Scroll to the **User Interface** section at the bottom of the notebook.\n",
        "2. The dashboard appears by default under the `Pipeline` tab.\n",
        "3. Select a ticker from the dropdown and click 'Run Pipeline'.\n",
        "4. Watch the model train and view output loss and average percentage error.\n",
        "5. Navigate to the `Visualization` tab explore various charts and performance metrics.\n",
        "6.Select a visualization and click 'Show' (some may take a few moments to load).\n",
        "7. Return to the `Pipeline` tab to reset the model and try a new ticker.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mdwih2aNY8Gy"
      },
      "source": [
        "## **Languages, Packages, & Libraries**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRV0SGSmZBlq"
      },
      "source": [
        "### Python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dP_3wZV4S5s0"
      },
      "outputs": [],
      "source": [
        "# !sudo update-alternatives --config python3\n",
        "!sudo apt-get install python3.10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2744W7-9vpTW"
      },
      "outputs": [],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5d7mGIRy2CJs"
      },
      "source": [
        "### Widgets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uKne1k5Q2G9U"
      },
      "outputs": [],
      "source": [
        "!pip install ipywidgets==8.1.1 --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0FsE1hU2M7q"
      },
      "outputs": [],
      "source": [
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2puvatVZHT1"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnuRlCppZlj2"
      },
      "source": [
        "#### Data Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2DenfTQHRcf3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import yfinance as yf\n",
        "from pandas_datareader import data as pdr\n",
        "import seaborn as sns\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evw8hCYcZrKd"
      },
      "source": [
        "#### Machine Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4yt3MBn6RrHy"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, root_mean_squared_error\n",
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import Input\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Dropout, Bidirectional\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YBeUMw1qS07s"
      },
      "outputs": [],
      "source": [
        "print('Tensorflow', tf.__version__)\n",
        "print('Keras', keras.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Seeds"
      ],
      "metadata": {
        "id": "HmH4Rpd-IW-B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set seeds for reproducibility\n",
        "def set_seed(seed=42):\n",
        "  np.random.seed(seed)\n",
        "  tf.random.set_seed(seed)\n",
        "  os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "set_seed(42)"
      ],
      "metadata": {
        "id": "jaRYLeJeIY50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-fEwImFZnTG"
      },
      "source": [
        "## **Exploratory Data Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgupF3GpW-GF"
      },
      "source": [
        "### Global Definitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLvRWu6bXIgc"
      },
      "outputs": [],
      "source": [
        "# default stock or index ticker\n",
        "ticker = 'DOW'\n",
        "\n",
        "# look-back window (days)\n",
        "lookback = 60\n",
        "\n",
        "# train and test split\n",
        "split_ratio = 0.8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38hzuJVnUCHY"
      },
      "source": [
        "### Create DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "in5dQqaWUSmX"
      },
      "outputs": [],
      "source": [
        "def load_data(ticker, start='2019-03-01', end='2025-03-01'):\n",
        "\n",
        "  \"\"\"\n",
        "  Downloads stock or market index data and merges with closing values for\n",
        "  Nasdaq and S&P 500.\n",
        "\n",
        "  Parameters:\n",
        "  - ticker: Ticker symbol chosen by user, default 'DOW'\n",
        "  - start: Starting date of data collection\n",
        "  - end: Ending date of data collection\n",
        "\n",
        "  Returns:\n",
        "  - df: Merged DataFrame\n",
        "  \"\"\"\n",
        "\n",
        "  # download individual stock data\n",
        "  df_stock = yf.download(ticker, start=start, end=end, progress=False)\n",
        "\n",
        "  # download market index data\n",
        "  indexes = ['NQ=F', 'SPGI']\n",
        "  df_indexes = {}\n",
        "\n",
        "  for index in indexes:\n",
        "    df_index = yf.download(index, start=start, end=end, progress=False)\n",
        "    # ensure df_indexes only has one level before merging\n",
        "    df_indexes[index] = df_index['Close'].squeeze()\n",
        "\n",
        "  # flatten the multiIndex (removes 'Ticker' level)\n",
        "  df_stock = df_stock.T.droplevel(1).T\n",
        "\n",
        "  # convert dictionary to dataframe before merging\n",
        "  df_indexes = pd.DataFrame(df_indexes)\n",
        "  df_indexes.rename(columns={'NQ=F': 'NQ_Close', 'SPGI': 'SPGI_Close'}, inplace=True)\n",
        "\n",
        "  df = df_stock.merge(df_indexes, on='Date', how='left')\n",
        "\n",
        "  return df\n",
        "\n",
        "load_data(ticker)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tI5_EJgaaf-J"
      },
      "source": [
        "## **Feature Engineering**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cH1VEmQbOnd"
      },
      "source": [
        "### Default Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_Z5tQVJbVgH"
      },
      "outputs": [],
      "source": [
        "def build_default_features(df):\n",
        "\n",
        "  \"\"\"\n",
        "  Adds technical indicators and features based on the individual stock.\n",
        "\n",
        "  Parameters:\n",
        "  - df: Merged DataFrame of stock/market data\n",
        "\n",
        "  Returns:\n",
        "  - df: Merged DataFrame with individual stock features\n",
        "  \"\"\"\n",
        "\n",
        "  # ----------------------------VOLUME------------------------------------\n",
        "  # moving averages: smooth out fluctuations and identify trends\n",
        "  df['MA_10'] = df['Close'].rolling(window=10).mean()\n",
        "  df['MA_50'] = df['Close'].rolling(window=50).mean()\n",
        "  df['MA_200'] = df['Close'].rolling(window=200).mean()\n",
        "\n",
        "  # on-balance volume: identifies buying and selling pressure\n",
        "  df['OBV'] = (np.sign(df['Close'].diff()) * df['Volume']).fillna(0).cumsum()\n",
        "\n",
        "  # volume-weighted avg price: avg price a stock traded at throughout the day\n",
        "  df['VWAP'] = (df['Close'] * df['Volume']).cumsum() / df['Volume'].cumsum()\n",
        "\n",
        "\n",
        "  # ----------------------------VOLATILITY---------------------------------\n",
        "  # rolling standard deviation: measures fluctuations over a given window\n",
        "  df['Rolling_std_10'] = df['Close'].rolling(window=10).std()\n",
        "  df['Rolling_std_20'] = df['Close'].rolling(window=20).std()\n",
        "\n",
        "  # Bollinger Bands: identify overbought/oversold conditions\n",
        "  df['BB_Middle'] = df['Close'].rolling(window=20).mean() # 20-day moving avg\n",
        "  df['BB_Upper'] = df['BB_Middle'] + (df['Rolling_std_20'] * 2)\n",
        "  df['BB_Lower'] = df['BB_Middle'] - (df['Rolling_std_20'] * 2)\n",
        "\n",
        "\n",
        "  # ------------------------------MOMENTUM---------------------------------\n",
        "  # rate of change: how much the price has changed over n days\n",
        "  df['ROC_10'] = df['Close'].pct_change(periods=10) * 100\n",
        "  df['ROC_20'] = df['Close'].pct_change(periods=20) * 100\n",
        "\n",
        "  # relative strength index\n",
        "  # df['RSI'] = df['Close']\n",
        "\n",
        "  # williams %R: used with BB to identify overbought/oversold conditions\n",
        "  df['WilliamsR'] = (df['High'].rolling(14).max() - df['Close']) / (\n",
        "      df['High'].rolling(14).max() - df['Low'].rolling(14).min()) * -100\n",
        "\n",
        "  df.dropna(inplace=True)\n",
        "\n",
        "  return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPPK_iglg1c7"
      },
      "source": [
        "### Market Index Features (Nasdaq, S&P Global)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TzANtYFQg_b6"
      },
      "outputs": [],
      "source": [
        "def build_market_features(df):\n",
        "\n",
        "  \"\"\"\n",
        "  Adds market-based indicators and correlation with the chosen stock.\n",
        "\n",
        "  Parameters:\n",
        "  - df: Merged DataFrame\n",
        "\n",
        "  Returns:\n",
        "  - df: Merged DataFrame with stock and market index features\n",
        "  \"\"\"\n",
        "\n",
        "  try:\n",
        "    # compute rolling correlation between stock, sp500, NASDAQ\n",
        "    df['Corr_NQ'] = df['Close'].rolling(50).corr(df['NQ_Close'])\n",
        "    df['Corr_SPGI'] = df['Close'].rolling(50).corr(df['SPGI_Close'])\n",
        "\n",
        "    # moving averages of market indices\n",
        "    df['NQ_MA_50'] = df['NQ_Close'].rolling(window=50).mean()\n",
        "    df['SPGI_MA_50'] = df['SPGI_Close'].rolling(window=50).mean()\n",
        "\n",
        "    # market rate of change\n",
        "    df['NQ_ROC_10'] = df['NQ_Close'].pct_change(periods=10) * 100\n",
        "    df['SPGI_ROC_10'] = df['SPGI_Close'].pct_change(periods=10) * 100\n",
        "\n",
        "    df.dropna(inplace=True)\n",
        "\n",
        "    # print('[DEBUG] Market features added.  Shape:', df.shape)\n",
        "\n",
        "    return df\n",
        "  except Exception as e:\n",
        "    print('[ERROR] Failed to add market features:', e)\n",
        "    return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kB41tGsoX0qh"
      },
      "source": [
        "### Global Definition of Full Feature Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Inu2lbIrX601"
      },
      "outputs": [],
      "source": [
        "FEATURES = ['Close', 'NQ_Close', 'SPGI_Close', 'MA_10', 'MA_50', 'MA_200',\n",
        "              'OBV', 'VWAP', 'Rolling_std_10', 'Rolling_std_20', 'BB_Middle',\n",
        "              'BB_Upper', 'BB_Lower', 'ROC_10', 'ROC_20', 'WilliamsR',\n",
        "              'Corr_NQ', 'Corr_SPGI', 'NQ_MA_50', 'SPGI_MA_50']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y07kEDxYiEqj"
      },
      "source": [
        "## **Data Preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iew5rXgKSWCE"
      },
      "source": [
        "### Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "etpQMPqWaLnN"
      },
      "outputs": [],
      "source": [
        "def prepare_data(ticker, start='2015-01-01', end='2025-01-01'):\n",
        "\n",
        "  \"\"\"\n",
        "  Loads stock and market index data for a given ticker and adds all features.\n",
        "\n",
        "  Parameters:\n",
        "  - ticker: Ticker symbol set by user, default 'AAPL'\n",
        "  - start: Starting date of data collection\n",
        "  - end: Ending date of data collection\n",
        "\n",
        "  Returns:\n",
        "  - df: Fully processed DataFrame\n",
        "  \"\"\"\n",
        "\n",
        "  df = load_data(ticker, start=start, end=end)\n",
        "  df = build_default_features(df)\n",
        "  df = build_market_features(df)\n",
        "\n",
        "  print('\\nStock and market data downloaded, DataFrame created.\\nIndividual stock features calculated.\\nMarket index features calculated.')\n",
        "\n",
        "  # clean extraneous columns\n",
        "  df = df.drop(['High', 'Low', 'Open', 'Volume'], axis=1)\n",
        "\n",
        "  print('\\nDataFrame ready for preprocessing.\\n')\n",
        "\n",
        "  return df\n",
        "\n",
        "df = prepare_data(ticker)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njlchEYzSzS6"
      },
      "source": [
        "### Preprocess Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jf74f565ich3"
      },
      "outputs": [],
      "source": [
        "def preprocess_data(df, lookback, split_ratio, features=FEATURES):\n",
        "\n",
        "  \"\"\"\n",
        "  Prepares the data for training by scaling relevant features down to values\n",
        "  between 0 and 1 for interpretability.  Splits the data into train and test\n",
        "  sets.\n",
        "\n",
        "  Parameters:\n",
        "  - df: Merged dataframe of stock/market data\n",
        "  - lookback: Globally-defined lookback window\n",
        "  - split_ratio: Globally defined split ratio for train and test sets\n",
        "  - features: Globally-defined feature set\n",
        "\n",
        "  Returns:\n",
        "  X_train, X_test, y_train, y_test, scaler, df_features\n",
        "  \"\"\"\n",
        "\n",
        "  scaler = MinMaxScaler()\n",
        "  df_features = df[features]\n",
        "\n",
        "  scaled_data = scaler.fit_transform(df_features)\n",
        "\n",
        "  X, y = [], []\n",
        "\n",
        "  for i in range(lookback, len(scaled_data)):\n",
        "    X.append(scaled_data[i - lookback:i])\n",
        "    y.append(scaled_data[i, 0]) # only predicting 'Close' price\n",
        "\n",
        "  # cast X and y sequence lists to arrays\n",
        "  X, y = np.array(X), np.array(y)\n",
        "  y = y.reshape(-1) # flatten to 1-D array\n",
        "\n",
        "  # split data into train and test sets along time axis\n",
        "  split_index = int(len(X)*split_ratio)\n",
        "  X_train, X_test = X[:split_index], X[split_index:]\n",
        "  y_train, y_test = y[:split_index], y[split_index:]\n",
        "\n",
        "  print('DataFrame preprocessed and ready for modeling.\\n\\n')\n",
        "\n",
        "  return X_train, X_test, y_train, y_test, scaler, df_features\n",
        "\n",
        "X_train, X_test, y_train, y_test, scaler, df = preprocess_data(df, lookback, split_ratio)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GboA3VdqAvMX"
      },
      "source": [
        "### Correlation Matrix of Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WbUoGAILA4vJ"
      },
      "outputs": [],
      "source": [
        "def show_feature_correlation(df):\n",
        "\n",
        "  \"\"\"\n",
        "  Plots a correlation matrix of the features after scaling.\n",
        "\n",
        "  Parameters:\n",
        "  - df: DataFrame of features from preprocess_data()\n",
        "  \"\"\"\n",
        "\n",
        "  corr_matrix = df.corr()\n",
        "  plt.figure(figsize=(10,8))\n",
        "  sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
        "  plt.title('Feature Correlation Matrix')\n",
        "  plt.show()\n",
        "\n",
        "show_feature_correlation(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNj48C8qlZFq"
      },
      "source": [
        "## **Modeling**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDQhE6ISlqgj"
      },
      "source": [
        "**Recall:**\n",
        "\n",
        "X_train.shape[0] --> N --> samples\n",
        "\n",
        "X_train.shape[1] --> T --> time\n",
        "\n",
        "X_train.shape[2] --> D --> features"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Configuration"
      ],
      "metadata": {
        "id": "rCXl4Lscepin"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model_config(ticker):\n",
        "  \"\"\"\n",
        "  Returns model hyperparameters based on whether the ticker is an index or\n",
        "  individual stock.\n",
        "  \"\"\"\n",
        "\n",
        "  stock_tickers = ['AAPL', 'FOX', 'JPM', 'NFLX', 'SPOT']\n",
        "  index_tickers = ['DOW', '^GSPC', '^IXIC', '^RUT']\n",
        "\n",
        "  # individual stock hyperparameter optimization\n",
        "  if ticker in stock_tickers:\n",
        "    config = {\n",
        "        # 'dense_unit_1': 25,\n",
        "        # 'dropout_1': 0.2,\n",
        "        # 'dense_unit_2': 25,\n",
        "        # 'dropout_2': 0.3\n",
        "\n",
        "        'dense_unit_1': 25,\n",
        "        'dropout_1': 0.2,\n",
        "        'dense_unit_2': 50,\n",
        "        'dropout_2': 0.3\n",
        "    }\n",
        "  elif ticker in index_tickers:\n",
        "    config = {\n",
        "        'dense_unit_1': 25,\n",
        "        'dropout_1': 0.1,\n",
        "        'dense_unit_2': 50,\n",
        "        'dropout_2': 0.1\n",
        "    }\n",
        "  else:\n",
        "    config = {'dropout_1': 0.2, 'dropout_2': 0.2,\n",
        "              'dense_unit_1': 25, 'dense_unit_2': 50}\n",
        "\n",
        "  print(f\"Training model for {ticker} with configuration: {config}\\n\\n\")\n",
        "  return config\n",
        "\n",
        "config = get_model_config(ticker)"
      ],
      "metadata": {
        "id": "wd4-0RV3eu9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZ-VqgJUUNdY"
      },
      "source": [
        "### Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KBUGnYqBmA41"
      },
      "outputs": [],
      "source": [
        "def train_lstm(X_train, y_train, X_test, y_test, epochs=75, batch_size=32,\n",
        "                                                              config=config):\n",
        "\n",
        "  \"\"\"\n",
        "  Trains and compiles a sequential model on an LSTM neural network with\n",
        "  time series data.\n",
        "\n",
        "  Parameters:\n",
        "  - X_train: Training set input features\n",
        "  - y_train: Training set actual values\n",
        "  - X_test: Test set input features\n",
        "  - y_test: Test set actual values\n",
        "  - epochs: Number of full iterations\n",
        "  - batch_size: Number of training samples processed in a single epoch\n",
        "\n",
        "  Returns:\n",
        "  model, history\n",
        "  \"\"\"\n",
        "\n",
        "  input_shape = (X_train.shape[1], X_train.shape[2])\n",
        "\n",
        "  model = Sequential([\n",
        "        Input(shape=input_shape),\n",
        "        LSTM(100, return_sequences=True),\n",
        "        Dense(config['dense_unit_1']),\n",
        "        Dropout(config['dropout_1']),\n",
        "        LSTM(50),\n",
        "        Dense(config['dense_unit_2']),\n",
        "        Dropout(config['dropout_2']),\n",
        "        Dense(1)\n",
        "    ])\n",
        "\n",
        "  model.compile(optimizer=Adam(learning_rate=0.0001), loss='mse')\n",
        "  history = model.fit(X_train, y_train, validation_data=(X_test, y_test),\n",
        "                                    epochs=epochs, batch_size=batch_size,\n",
        "                                                              verbose=2)\n",
        "\n",
        "  return model, history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8V9LJB9UmW9"
      },
      "source": [
        "### Train and Compile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LA0vFEQE-F-B"
      },
      "outputs": [],
      "source": [
        "lstm_model, history = train_lstm(X_train, y_train, X_test, y_test)\n",
        "lstm_predictions = lstm_model.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Summary"
      ],
      "metadata": {
        "id": "W5jzdbP6vozN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_model_summary(model):\n",
        "  model.summary()\n",
        "\n",
        "summary = print_model_summary(lstm_model)"
      ],
      "metadata": {
        "id": "L_mGlQr1vrkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Validation**"
      ],
      "metadata": {
        "id": "Lj4JSAgrYP_l"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6Y57dgLCUdj"
      },
      "source": [
        "### Calculate Loss (MSE, RMSE, MAE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GO9zAwW5gUE1"
      },
      "outputs": [],
      "source": [
        "def calculate_loss(y_test, lstm_predictions):\n",
        "\n",
        "  mse = mean_squared_error(y_test, lstm_predictions)\n",
        "  rmse = root_mean_squared_error(y_test, lstm_predictions)\n",
        "  mae = mean_absolute_error(y_test, lstm_predictions)\n",
        "\n",
        "  print(f'\\nMSE: {mse}')\n",
        "  print(f'RMSE: {rmse}')\n",
        "  print(f'MAE: {mae}\\n')\n",
        "\n",
        "calculate_loss(y_test, lstm_predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3GZkfkkUa_4"
      },
      "source": [
        "### Plot Validation Loss (MSE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zzgkx-iJ9sIG"
      },
      "outputs": [],
      "source": [
        "def plot_training_history(history):\n",
        "\n",
        "  \"\"\"\n",
        "  Plots the training and validation loss for each epoch.\n",
        "\n",
        "  Parameters:\n",
        "  - history: fitted model\n",
        "  \"\"\"\n",
        "\n",
        "  plt.figure(figsize=(10,5))\n",
        "  plt.plot(history.history['loss'], label='loss')\n",
        "  plt.plot(history.history['val_loss'], label='val_loss')\n",
        "\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.title('Training vs Validation Loss')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "plot_training_history(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9Yz0-WbDDt4"
      },
      "source": [
        "## **Evaluation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cv5gfJjAaHOn"
      },
      "source": [
        "### Feature Importance -- Permutation Method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IGvUN9p0aKTr"
      },
      "outputs": [],
      "source": [
        "def permutation_feature_importance(model, X_test, y_test, features=FEATURES):\n",
        "\n",
        "  \"\"\"\n",
        "  Evaluates feature importance by measuring the increase in MSE after shuffling\n",
        "  each feature in X_test.\n",
        "\n",
        "  Parameters:\n",
        "  - model: Trained LSTM model\n",
        "  - X_test: Test set input features\n",
        "  - y_test: Test set actual values\n",
        "  - features: Global feature set\n",
        "\n",
        "  Returns:\n",
        "  df of features sorted by importance\n",
        "  \"\"\"\n",
        "\n",
        "  baseline_pred = model.predict(X_test, verbose=0)\n",
        "  baseline_mse = mean_squared_error(y_test, baseline_pred)\n",
        "\n",
        "  importances = {}\n",
        "\n",
        "  for i, feature in enumerate(features):\n",
        "    X_permuted = np.copy(X_test)\n",
        "\n",
        "    # shuffle values for this feature across all samples and timesteps\n",
        "    for t in range(X_permuted.shape[1]):\n",
        "      np.random.shuffle(X_permuted[:, t, i])\n",
        "\n",
        "    permuted_pred = model.predict(X_permuted, verbose=0)\n",
        "    permuted_mse = mean_squared_error(y_test, permuted_pred)\n",
        "\n",
        "    # importance is the increase in error caused by shuffling this feature\n",
        "    importances[feature] = permuted_mse - baseline_mse\n",
        "\n",
        "  # sort and display results\n",
        "  importance_df = pd.DataFrame(importances.items(), columns=['Feature', 'Importance']\n",
        "                               ).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "  # print(\"\\nFeature Importance (Permutation Method):\")\n",
        "  # print(importance_df)\n",
        "\n",
        "  return importance_df\n",
        "\n",
        "importance_df = permutation_feature_importance(lstm_model, X_test, y_test, FEATURES)\n",
        "print(importance_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0qG_1u6oACV"
      },
      "source": [
        "### Plot Top Feature Importances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_CY8LSooFXQ"
      },
      "outputs": [],
      "source": [
        "def plot_feature_importance(importance_df, top_n=10):\n",
        "\n",
        "  plt.figure(figsize=(10,6))\n",
        "  sns.barplot(data=importance_df.head(top_n), x='Importance', y='Feature', hue='Feature',\n",
        "              palette='crest', legend=False)\n",
        "  plt.title('Top Feature Importances (Permutation Method)')\n",
        "  plt.xlabel('MSE Increase on Feature Shuffle')\n",
        "  plt.ylabel('Feature')\n",
        "  plt.grid(True, axis='x')\n",
        "  plt.tight_layout()\n",
        "  plt.show();\n",
        "\n",
        "plot_feature_importance(importance_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9eJo9gyVNCg"
      },
      "source": [
        "### Inverse Transform Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ASGcmJ3rDt4Z"
      },
      "outputs": [],
      "source": [
        "def inverse_transform(y_test, predictions, scaler, features=FEATURES):\n",
        "\n",
        "  \"\"\"\n",
        "  Inverses the scaling of predictions back to original prices.\n",
        "\n",
        "  Parameters:\n",
        "  - y_test: Ground truth values (scaled)\n",
        "  - predictions: Predicted values from model (scaled)\n",
        "  - scaler: the fitted MinMaxScaler used on the feature set\n",
        "  - features: list of features used during scaling\n",
        "\n",
        "  Returns:\n",
        "  true_close, adj_close\n",
        "  \"\"\"\n",
        "\n",
        "  # modify scaler.inverse_transform() to include features\n",
        "  y_test_rescaled = np.zeros((y_test.shape[0], len(features)))\n",
        "  y_test_rescaled[:,0] = y_test\n",
        "\n",
        "  # modify scaler.inverse_transform() to include features\n",
        "  preds_rescaled = np.zeros((predictions.shape[0], len(features)))\n",
        "  preds_rescaled[:,0] = predictions[:,0]\n",
        "\n",
        "  # extract the first column (closing price)\n",
        "  adj_close = scaler.inverse_transform(preds_rescaled)[:,0]\n",
        "  true_close = scaler.inverse_transform(y_test_rescaled)[:,0]\n",
        "\n",
        "  return true_close, adj_close\n",
        "\n",
        "true_prices, predicted_prices = inverse_transform(y_test, lstm_predictions, scaler)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculate Error"
      ],
      "metadata": {
        "id": "ESLy-q4Jy-fv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_error(true, adj):\n",
        "  pct_error = np.mean(np.abs(true-adj) / true) * 100\n",
        "  print(f'\\nAverage Percentage Error: {pct_error:.2f}%\\n')\n",
        "\n",
        "  return pct_error\n",
        "\n",
        "error = compute_error(true_prices, predicted_prices)"
      ],
      "metadata": {
        "id": "1oFCoeQ-zQcM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dwwZq-AVS5M"
      },
      "source": [
        "### Plot True vs Predicted Prices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02USEN9MJn0t"
      },
      "outputs": [],
      "source": [
        "def plot_predictions_with_dates(df, true_prices, predicted_prices, lookback,\n",
        "                                                              split_ratio):\n",
        "\n",
        "  \"\"\"\n",
        "  Plots actual vs. predicted stock prices over time with proper date labels.\n",
        "\n",
        "  Parameters:\n",
        "  - df: Original DataFrame with full date index\n",
        "  - true_prices: Actual prices after inverse transform\n",
        "  - predicted_prices: Model predictions after inverse transform\n",
        "  - lookback: number of days in input window\n",
        "  - split_ratio: train/test split ratio\n",
        "  \"\"\"\n",
        "\n",
        "  # align with end of test set\n",
        "  total_samples = lookback + len(true_prices)\n",
        "  dates = df.index[-len(true_prices):]\n",
        "\n",
        "  plt.figure(figsize=(12,6))\n",
        "  plt.plot(dates, true_prices, label='True Prices')\n",
        "  plt.plot(dates, predicted_prices, label='Predicted Prices')\n",
        "  plt.xlabel('Date')\n",
        "  plt.ylabel('Price')\n",
        "  plt.title('LSTM Stock Price Predictions')\n",
        "  plt.legend()\n",
        "  plt.grid(True)\n",
        "  plt.tight_layout()\n",
        "  plt.xticks(rotation=45)\n",
        "  plt.show()\n",
        "\n",
        "plot_predictions_with_dates(df, true_prices, predicted_prices, lookback, split_ratio)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DataFrame of Closing Prices"
      ],
      "metadata": {
        "id": "obubrWzNWKHG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_prices(df, y_test, true_prices, predicted_prices):\n",
        "\n",
        "  df_close = pd.DataFrame({\n",
        "      'Date': df.index[-len(y_test):],\n",
        "      'True Close': true_prices,\n",
        "      'Adjusted Close': predicted_prices,\n",
        "  })\n",
        "\n",
        "  print('\\nDataFrame of True vs Predicted Prices\\n')\n",
        "  print(df_close)\n",
        "\n",
        "compare_prices(df, y_test, true_prices, predicted_prices)"
      ],
      "metadata": {
        "id": "85UzQwfGWPmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsP49lP7lHcC"
      },
      "source": [
        "## **Pipeline Script**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run_prediction_pipeline(ticker):\n",
        "    \"\"\"\n",
        "    Loads and preprocesses data, trains the model, and generates predictions.\n",
        "    Saves results to the global `state` dictionary and key variables to the global namespace.\n",
        "    Returns the full `state` for optional inspection.\n",
        "    \"\"\"\n",
        "\n",
        "    global df, history, X_test, y_test, true_prices, predicted_prices, model, error\n",
        "\n",
        "    print(f\"\\nRunning LSTM prediction for {ticker}...\\n\")\n",
        "\n",
        "    # Load and preprocess\n",
        "    df = prepare_data(ticker)\n",
        "    X_train, X_test, y_train, y_test, scaler, df = preprocess_data(df, lookback, split_ratio)\n",
        "\n",
        "    # Train model\n",
        "    model_config = get_model_config(ticker)\n",
        "    model, history = train_lstm(X_train, y_train, X_test, y_test, config=model_config)\n",
        "\n",
        "    # Predict\n",
        "    predictions = model.predict(X_test)\n",
        "\n",
        "    # Validate\n",
        "    loss = calculate_loss(y_test, predictions)\n",
        "\n",
        "    # Evaluate\n",
        "    true_prices, predicted_prices = inverse_transform(y_test, predictions, scaler)\n",
        "    error = compute_error(true_prices, predicted_prices)\n",
        "\n",
        "    return df, history, X_test, y_test, true_prices, predicted_prices, error"
      ],
      "metadata": {
        "id": "mdDzX5neKsyF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z719mlNB2bu8"
      },
      "source": [
        "## **User Interface**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "state = {\n",
        "    'df': None,\n",
        "    'X_test': None,\n",
        "    'y_test': None,\n",
        "    'true_prices': None,\n",
        "    'predicted_prices': None,\n",
        "    'model': None,\n",
        "    'history': None,\n",
        "    'error': None,\n",
        "    'feature_list': None\n",
        "}\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "from ipywidgets import VBox, HBox, Tab\n",
        "import io\n",
        "from contextlib import redirect_stdout\n",
        "\n",
        "# --- Global State ---\n",
        "pipeline_output = widgets.Output()\n",
        "viz_output = widgets.Output()\n",
        "# df = model = history = true_prices = predicted_prices = y_test = X_test = None\n",
        "\n",
        "# --- Widgets ---\n",
        "ticker_dropdown = widgets.Dropdown(\n",
        "    options=['AAPL', 'AXP', 'DOW', 'FOX', 'GOOGL', '^GSPC', '^IXIC', 'JPM', 'LMT', 'MA', 'MSFT',\n",
        "             'NFLX', '^RUT', 'SPOT'],\n",
        "    value='DOW',\n",
        "    description='Ticker:',\n",
        "    layout=widgets.Layout(width='200px')\n",
        ")\n",
        "\n",
        "run_button = widgets.Button(\n",
        "    description='Run Pipeline',\n",
        "    button_style='success',\n",
        "    layout=widgets.Layout(width='150px')\n",
        ")\n",
        "\n",
        "viz_dropdown = widgets.Dropdown(\n",
        "    options=[\n",
        "        'Feature Correlation Matrix',\n",
        "        'Model Summary',\n",
        "        'Training vs Validation Loss',\n",
        "        'Feature Importance',\n",
        "        'True vs Predicted Prices',\n",
        "        'Price Comparison Table'\n",
        "    ],\n",
        "    description='Visual:',\n",
        "    layout=widgets.Layout(width='350px')\n",
        ")\n",
        "\n",
        "show_button = widgets.Button(\n",
        "    description='Show',\n",
        "    button_style='info',\n",
        "    layout=widgets.Layout(width='100px')\n",
        ")\n",
        "\n",
        "reset_button = widgets.Button(\n",
        "    description='Reset All',\n",
        "    button_style='danger',\n",
        "    layout=widgets.Layout(width='120px')\n",
        ")\n",
        "\n",
        "# --- Utility: Get Model Summary as Text (avoid infinite loop) ---\n",
        "def get_model_summary_text(model):\n",
        "    buf = io.StringIO()\n",
        "    with redirect_stdout(buf):\n",
        "        model.summary()\n",
        "    return buf.getvalue()\n",
        "\n",
        "# --- Callbacks ---\n",
        "def on_run_click(b):\n",
        "    with pipeline_output:\n",
        "        clear_output(wait=True)\n",
        "        ticker = ticker_dropdown.value\n",
        "\n",
        "        df, history, X_test, y_test, true_prices, predicted_prices, error = run_prediction_pipeline(ticker)\n",
        "\n",
        "        state.update({\n",
        "            'df': df,\n",
        "            'history': history,\n",
        "            'y_test': y_test,\n",
        "            'true_prices': true_prices,\n",
        "            'predicted_prices': predicted_prices,\n",
        "            'error': error,\n",
        "            'model': model,\n",
        "            'X_test': X_test,\n",
        "            'feature_list': FEATURES\n",
        "        })\n",
        "\n",
        "        print(f\"Model trained for {ticker} with {len(df)} data points.\")\n",
        "\n",
        "\n",
        "def on_show_click(b):\n",
        "\n",
        "    with viz_output:\n",
        "        clear_output(wait=True)\n",
        "        if state['model'] is None:\n",
        "            print(\"⚠️ Please run the pipeline first.\")\n",
        "            return\n",
        "\n",
        "        choice = viz_dropdown.value\n",
        "        if choice == 'Model Summary':\n",
        "            print(get_model_summary_text(state['model']))\n",
        "        elif choice == 'True vs Predicted Prices':\n",
        "            plot_predictions_with_dates(state['df'], state['true_prices'], state['predicted_prices'], lookback, split_ratio)\n",
        "        elif choice == 'Feature Importance':\n",
        "            importance_df = permutation_feature_importance(state['model'], state['X_test'], state['y_test'], state['feature_list'])\n",
        "            plot_feature_importance(importance_df)\n",
        "        elif choice == 'Training vs Validation Loss':\n",
        "            plot_training_history(state['history'])\n",
        "        elif choice == 'Feature Correlation Matrix':\n",
        "            show_feature_correlation(state['df'])\n",
        "        elif choice == 'Price Comparison Table':\n",
        "            compare_prices(state['df'], state['y_test'], state['true_prices'], state['predicted_prices'])\n",
        "        else:\n",
        "            print(\"Select a visualization.\")\n",
        "\n",
        "\n",
        "def on_reset_click(b):\n",
        "    for key in state:\n",
        "        state[key] = None\n",
        "    with pipeline_output:\n",
        "        clear_output()\n",
        "        print(\"Pipeline output cleared.\")\n",
        "    with viz_output:\n",
        "        clear_output()\n",
        "        print(\"Visualization output cleared.\")\n",
        "\n",
        "# --- Connect Buttons ---\n",
        "run_button.on_click(on_run_click)\n",
        "show_button.on_click(on_show_click)\n",
        "reset_button.on_click(on_reset_click)\n",
        "\n",
        "# --- Layout ---\n",
        "pipeline_tab = VBox([\n",
        "    HBox([ticker_dropdown, run_button, reset_button]),\n",
        "    pipeline_output\n",
        "])\n",
        "\n",
        "visual_tab = VBox([\n",
        "    HBox([viz_dropdown, show_button]),\n",
        "    viz_output\n",
        "])\n",
        "\n",
        "tabs = Tab(children=[pipeline_tab, visual_tab])\n",
        "tabs.set_title(0, 'Pipeline')\n",
        "tabs.set_title(1, 'Visualization')\n",
        "\n",
        "# --- Display UI ---\n",
        "display(tabs)\n"
      ],
      "metadata": {
        "id": "uZ3Uhq71LNqC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "Mdwih2aNY8Gy",
        "Z-fEwImFZnTG",
        "38hzuJVnUCHY",
        "tI5_EJgaaf-J",
        "1cH1VEmQbOnd",
        "KPPK_iglg1c7",
        "y07kEDxYiEqj",
        "iew5rXgKSWCE",
        "T8V9LJB9UmW9",
        "Lj4JSAgrYP_l",
        "f6Y57dgLCUdj",
        "G3GZkfkkUa_4",
        "H9Yz0-WbDDt4",
        "r0qG_1u6oACV",
        "o9eJo9gyVNCg",
        "ESLy-q4Jy-fv",
        "1dwwZq-AVS5M",
        "obubrWzNWKHG",
        "NsP49lP7lHcC"
      ],
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyN8nub9lBfBj6B3/51S4maP",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}